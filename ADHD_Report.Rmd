---
title: "ADHD Report"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading the train dataset

```{r}
DATA <-  read.csv("train_data.csv")
adhd0 <- t(DATA)
adhd1 <- adhd0[-1, ]

#naming the colnames
colnames(adhd1) <- as.character(adhd1[1, ])
X <- apply(adhd1[,-1],2, as.numeric)
X <- data.frame(adhd1[ ,1],X)
adhd2 <- X[-1, ]
adhd3 <- as.matrix(adhd2)
colnames(adhd3)[1] <- "GROUP"

#defining control group=0  
cat_v <- c()
for (i in 1:dim(adhd3)[1]){
  if(adhd3[ ,"GROUP"][i] == "Controls")
  {
    cat_v[i]<-"0"
  }else{
    cat_v[i]<-"1"
  }
}
#as.factor(cat_v)
adhd3 <- data.frame(cat_v, adhd3)
Z <- adhd3[ ,-2]
Z <- apply(Z,2,as.numeric)
r <- Z[ ,1]
r <- as.matrix(r)
adhd <- Z[ ,-1]

rm(list=c("adhd0", "adhd1", "adhd2", "adhd3", "Z"))
```

## Splitting the training dataset

```{r}
#training and testing data
## % 80 of the sample size
ind <- floor(0.8 * nrow(adhd))
set.seed(2705)
train_ind <- sample(seq_len(nrow(adhd)), size = ind)

x_train <- as.data.frame(adhd[train_ind, ])
y_train <- as.factor(r[train_ind, ])
x_test <- as.data.frame(adhd[-train_ind, ])
y_test <- as.factor(r[-train_ind, ])
```

## Loading the Original test dataset
```{r}
#test dataset

DATA1 <-  read.csv("test_data.csv")
ADHD0 <- t(DATA1)
ADHD1 <- ADHD0[-1, ]

#naming the colnames
colnames(ADHD1) <- as.character(ADHD1[1, ])
X_t <- apply(ADHD1[,-1],2, as.numeric)
X_t <- data.frame(ADHD1[ ,1],X_t)
ADHD2 <- X_t[-1, ]
ADHD3 <- as.matrix(ADHD2)
colnames(ADHD3)[1] <- "group"

#defining control group=0  
cat_t <- c()
for (i in 1:dim(ADHD3)[1]){
  if(ADHD3[ ,"group"][i] == "Controls")
  {
    cat_t[i]<-"0"
  }else{
    cat_t[i]<-"1"
  }
}
ADHD3 <- data.frame(cat_t, ADHD3)
Y<- ADHD3[ ,-2]
Y <- apply(Y,2,as.numeric)
colnames(Y)[1] <- "Group_t"

rm(list=c("ADHD0", "ADHD1", "ADHD2", "ADHD3"))
```

## Loading the SWAG Logistic Regression results

```{r message=FALSE, warning=FALSE}
load("ADHD_package.Rda")
```

## Post-processing
```{r}
min_error = min(train_swag_svml$cv_alpha)


varmat_ind = list() #saves the varmat index from CV errors
for(i in 1:20){
  varmat_ind[[i]]=which(train_swag_svml$CVs[[i]]<=min_error, arr.ind = T)
}

post_sel = list() # models selected after post-processing
for(i in 1:20){
  post_sel[[i]] = train_swag_svml$VarMat[[i]][,varmat_ind[[i]]]
}

post_sel[[1]] = t(as.matrix(post_sel[[1]]))

for(i in 1:20){
  if(!is.matrix(post_sel[[i]])){
    post_sel[[i]]=t(as.matrix(post_sel[[i]]))
  }
}

post_sel[[9]] = t(post_sel[[9]])


x = c() #non-empty elements of post-group
for(i in 1:20){
  if(length(post_sel[[i]])!=0){
    x = c(x,i)
  }
  x = x
}

for(i in 1:length(x)){
  if(nrow(post_sel[[x[i]]])<20){
    diff = 20 - nrow(post_sel[[x[i]]])
    post_sel[[x[i]]] = rbind(post_sel[[x[i]]],matrix(NA,nrow = diff, ncol = ncol(post_sel[[x[i]]])))
  } 
}


models = matrix(NA, nrow = 20,ncol=ncol(post_sel[[x[1]]]))

models[1:nrow(post_sel[[x[1]]]),]=post_sel[[x[1]]]

for(i in 2:length(x)){
  models = cbind(models,post_sel[[x[i]]])
}

models = t(models)

selected_var = c() 
for(i in 1:ncol(models)){
  selected_var = c(selected_var,models[,i])
}
selected_var = na.omit(unique(selected_var))
selected_var = sort(selected_var)
#colnames(x_train)[selected_var]
#table(models)

freq = table(models)
variable = colnames(x_train)[selected_var]

freq_table = cbind(variable,freq)
rownames(freq_table) = c(1:nrow(freq_table))
freq_table = as.data.frame(freq_table)
freq_table$freq = as.numeric(freq_table$freq)

freq_table
```

|-|Variable      | Frequency| 
|-| -------------- | ------- |
| 1| 13.11.      | 51|
| 2|  27.21.      | 51|
| 3|    40.19.  | 3|
| 4|   47.9.  | 5|
| 5|   47.21.| 258|
| 6|  47.27.| 1|
| 7|  49.9.| 3|
| 8|   49.47.| 21|
| 9|  54.21.      | 381|
| 10|  72.11.    | 363|
| 11|  72.68.  | 381|
| 12| 74.21.  | 43|
| 13|  74.55.| 63|
|14|82.13.| 12|
|15|85.13.| 8|
|16|101.85.| 381|
|17| 104.34.      | 377|
|18| 108.27.     | 207|
| 19|108.47.  | 73|
| 20|108.91.  | 201|
| 21|112.67.| 7|
|22| 114.6.| 23|
|23| 114.28.| 172|
|24| 114.40.| 281|
|25| 116.21.      | 45|
| 26| 116.47.      | 23|
| 27| 116.55.  | 84|
| 28| 116.89.  | 381|
|29|  124.23.| 28|
|30| 124.99.| 284|
|31| 126.2.| 325|
|32| 126.124.| 18|
| 33| 129.40.      | 25|
|34|  132.85.      | 1|
|35| 134.23.  | 46|
|36| 134.99.  | 66|
|37|  135.114.| 1|
|38| 136.132.| 1|
|39| 138.23.| 20|
|40| 138.107.| 9|
|41| 146.89.      | 235|
|42|  148.71.      | 9|
|43|  150.114.  | 363|
|44|  151.46.  | 55|
|45|  151.147.| 377|
|46| 162.73.| 267|
|47| 169.49.| 34|
|48| 173.9.| 149|
|49|  173.47.      | 13|
|50|  184.58.      | 37|
|51|  189.21.  | 1|
|52|  189.65.  | 7|
|53|  189.173.| 37|

`models` matrix contains the models and 53 variables chosen after post-processing. There are 381 models of dimensions starting from 9 upto 20. freq_table shows the frequency of each feature selected. The most frequent features are `54.21.`, `72.68.`, `101.85.` and `116.89.`,  which are contained by every model. The most second frequent ones are `104.34.`, `151.147.` frequency 377.

```{r message=FALSE, warning=FALSE}
require(plotrix)

m_vector <- sapply(train_swag_svml$CVs, function(x) summary(x)[4])

l_vector <- sapply(train_swag_svml$CVs, function(x) summary(x)[1])

u_vector <- sapply(train_swag_svml$CVs, function(x) summary(x)[6])

plotCI(1:length(train_swag_svml$CVs), m_vector, ui=u_vector, li=l_vector, scol = "grey", col="red", pch = 16, main = "Ranges 10-fold CV Misclassification Errors",ylab = "Range CV Error",xlab = "Model Size")
```

The plot shows the range of CV errors for each dimension. There is a clear decay in CV errors as the dimension increases.




## Loading the SWAG SVM Linear results

```{r}
load("SVMLinear_Results.Rda")
```

## FREQUENCY TABLE

In the post-processing, we took the minimum CV_alpha and selected all the models who have CV_alpha less than or equal to it. The minimum CV_alpha is 0.220925 and is the CV of dimension 20.

There are a total of 114 models of dimensions 18, 19, 20. "54,21", "91,85", "105,10", "121,68", "126,2", "135,105", "140,53", "150,114", "152,22", "154,112", "174,73" features are present in all the models. Below is the frequency table.

\begin{table}
\centering
\begin{tabular}[t]{c|c}
\hline
Feature & Frequency \\
\hline
14,6 &	5 \\
\hline
24,6 &	1 \\
\hline
54,21 &	114\\
\hline
58,38 &	4\\
\hline
58,47 &	2\\
\hline
61,31 &	1\\
\hline
61,35 &	6\\
\hline
62,10	& 8\\
\hline
69,15	& 1\\
\hline
70,14	& 66\\
\hline
72,6	& 12\\
\hline
77,4	& 9\\
\hline
77,19	& 2\\
\hline
78,58	& 40\\
\hline
79,13	& 7\\
\hline
80,77	& 12\\
\hline
82,69	& 5\\
\hline
82,80	& 1\\
\hline
83,27	& 2\\
\hline
88,45	& 6\\
\hline
91,85	& 114\\
\hline
92,2	& 2\\
\hline
92,73	& 39\\
\hline
95,53	& 1\\
\hline
99,14	& 1\\
\hline
103,91 & 1\\
\hline
104,21 &	1\\
\hline
104,90 &	3\\
\hline
105,10 & 114\\
\hline
105,83 &	4\\
\hline
105,91 &	71\\
\hline
108,10 &	4\\
\hline
113,107 &	1\\
\hline
114,68 &	8\\
\hline
114,72 &	1\\
\hline
115,54 &	1\\
\hline
116,30 &	3\\
\hline
116,42 &	3\\
\hline
116,95 &	1\\
\hline
117,102 &	3\\
\hline
119,99 &	1\\
\hline
121,68 &	114\\
\hline
124,34 &	1\\
\hline
126,2	& 114\\
\hline
128,116 &	3\\
\hline
130,80 &	66\\
\hline
135,55 &	1\\
\hline
135,82 &	3\\
\hline
135,105 &	114\\
\hline
137,9	& 4\\
\hline
137,131 &	9\\
\hline
138,89 &	6\\
\hline
138,92 &	1\\
\hline
139,20 &	1\\
\hline
139,118 &	3\\
\hline
140,53 &	114\\
\hline
145,18 &	3\\
\hline
146,30 &	5\\
\hline
146,31 &	1\\
\hline
146,78 &	1\\
\hline
147,33 &	1\\
\hline
147,65 &	16\\
\hline
148,14 &	28\\
\hline
148,47 &	1\\
\hline
148,89 &	1\\
\hline
148,146 &	6\\
\hline
150,40 &	67\\
\hline
150,114 &	114\\
\hline
151,147 &	2\\
\hline
152,22 &	114\\
\hline
154,112 &	114\\
\hline
155,7	& 5\\
\hline
157,81 &	56\\
\hline
160,62 &	1\\
\hline
161,47 &	66\\
\hline
163,55 &	1\\
\hline
164,82 &	4\\
\hline
165,23 &	4\\
\hline
165,135 &	27\\
\hline
166,112 &	1\\
\hline
168,34 &	3\\
\hline
168,97 &	2\\
\hline
168,116 &	1\\
\hline
169,18 &	10\\
\hline
169,47 &	15\\
\hline
169,108 &	1\\
\hline
171,59 &	1\\
\hline
172,136 &	1\\
\hline
174,57 &	4\\
\hline
174,73 &	114\\
\hline
175,27 &	1\\
\hline
177,55 &	2\\
\hline
177,65 &	2\\
\hline
179,47 &	2\\
\hline
183,12 &	106\\
\hline
183,21 &	1\\
\hline
183,118 &	2\\
\hline
185,34 &	24\\
\hline
185,62 &	1\\
\hline
187,14 &	11\\
\hline
187,60 &	1\\
\hline
188,35 &	5\\
\hline
188,99 &	5\\
\hline
188,174 &	5\\
\hline
189,8	& 22\\
\hline
189,34 &	7\\
\hline
189,64 &	5\\
\hline
189,99 &	2\\
\hline
190,3	& 5\\
\hline
190,127 &	10\\
\hline
\end{tabular}
\end{table}

